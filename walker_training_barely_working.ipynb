{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c99faf649cd52b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Teaching a quadruped to walk\n",
    "\n",
    "Time to try out the learning algorithms that you just implemented on a more difficult problem. The WalkerEnv implements a quadruped robot kind-of thing, see for yourself. The goal is to move in the $x$ direction as fast and as far as possible.\n",
    "\n",
    "Your goal is to implement a class `WalkerPolicy` with function `determine_actions()` just like the StochasticPolicy we used earlier to control the pendulum. Below is a template of this class, but feel free to alter it however you want. The only important thing is the `determine_actions()` function!\n",
    "\n",
    "After you implement it, copy `WalkerPolicy` into a separate file `WalkerPolicy.py` that you will upload to BRUTE together with the (optional) learned weights in a zip file. How the policy is implemented is up to you! You are constrained to only the libraries we used so far though, such as torch, numpy etc..\n",
    "\n",
    "You will get some free points just for uploading a working policy (irrelevant of the performance). Further 2 points will be awarded for successfully traversing a small distance in the x direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41290d3f9ccf033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hints\n",
    "\n",
    "There is no single easy way of doing this, but here are some suggestions on what you could try to improve your policy:\n",
    "\n",
    "1. This problem is much more difficult, than balancing a pendulum. It is a good idea to use a bit larger network than for the pendulum policy.\n",
    "\n",
    "2. You can also try to use a different optimizer, such as Adam and play with the hyperparameters.\n",
    "\n",
    "3. Using a neural network to compute the normal distribution scale $\\sigma$ can lead to too much randomness in the actions (i.e. exploration). You can use a fixed $\\sigma$ instead, or replace it with a learnable `torch.Parameter` initialized to some small constant. Make sure, you run it through an exponential, or softplus function to ensure $\\sigma$ is positive.\n",
    "\n",
    "4. The exploration can also be reduced by penalizing the variance of the action distribution in an additional loss term.\n",
    "\n",
    "5. If you see some undesirable behaviour, you can tweak the reward function to penalize it. Even though the $x$ distance is all we care about, adding extra terms to the reward can help guide the learning process (This is known as reward shaping). Simply define a reward function mapping the state $s_{t+1}$ and action $a_t$ to a scalar reward $r_t$ and put it in the config dictionary under the key `'reward_fcn'`. See the `WalkerEnv` class for the implementation of the default reward.\n",
    "\n",
    "6. Using the normal distribution on a bounded action space can lead to certain problems caused by action clipping. This can be mitigated by using a different distribution, such as the Beta distribution. See the `torch.distributions.beta` module for more information. (Note that Beta distribution is defined on the interval [0,1] and works better with parameters $\\alpha,\\beta \\geq 1$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot run with the visualization, you can set this to False\n",
    "VISUALIZE = True\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec37af",
   "metadata": {},
   "source": [
    "### import self-made modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52d6512e1dc81e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:30:41.415964800Z",
     "start_time": "2023-11-30T16:30:40.816557700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy\n",
    "import solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6646694",
   "metadata": {},
   "source": [
    "### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac22145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_MULTIPLIER = 0\n",
    "VELOCITY_MULTIPLIER = 1\n",
    "ACTION_PENALTY_MULTIPLIER = 0.001\n",
    "STABILITY_PENALTY_MULTIPLIER = 0.001\n",
    "\n",
    "def walker_reward(state, action):\n",
    "    pos = state[:15]  # first 15 elements of state vector are generalized coordinates [xyz, quat, joint_angles]\n",
    "    vel = state[15:]  # last 14 elements of state vector are generalized velocities [xyz_vel, omega, joint_velocities]\n",
    "    x_velocity = vel[0]  # this is the x axis velocity\n",
    "    x_distance = pos[0]  # this is the x axis position\n",
    "    stability_penalty = np.sum(np.abs(vel[1:3]))  # Penalize y and z velocities\n",
    "    action_penalty = np.sum(np.square(action))  # Penalize large actions\n",
    "    return (x_distance * DISTANCE_MULTIPLIER + \n",
    "            x_velocity * VELOCITY_MULTIPLIER - \n",
    "            STABILITY_PENALTY_MULTIPLIER * stability_penalty - \n",
    "            ACTION_PENALTY_MULTIPLIER * action_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1463d",
   "metadata": {},
   "source": [
    "## Train loop\n",
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd317d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectories(env, policy, T, device='cpu'):\n",
    "    \"\"\"\n",
    "    Collect T steps of experience from N parallel walkers in 'env'.\n",
    "    Returns:\n",
    "      states:  [T+1, N, state_dim]\n",
    "      actions: [T,   N, action_dim]\n",
    "      rewards: [T,   N]\n",
    "    \"\"\"\n",
    "    # 1) Reset environment\n",
    "    obs_list = env.reset()  # list of length N, each shape(29,)\n",
    "    obs_list = obs_list[0]\n",
    "    N = len(obs_list)\n",
    "    state_dim = len(obs_list[0])\n",
    "    action_dim = 8  # for your quadruped\n",
    "\n",
    "    # Convert to torch\n",
    "    obs = torch.tensor(np.stack(obs_list), dtype=torch.float32, device=device)  # [N, 29]\n",
    "\n",
    "    # 2) Allocate buffers\n",
    "    states  = torch.zeros((T+1, N, state_dim),  device=device)\n",
    "    actions = torch.zeros((T,   N, action_dim), device=device)\n",
    "    rewards = torch.zeros((T,   N),            device=device)\n",
    "\n",
    "    states[0] = obs\n",
    "\n",
    "    # 3) Roll out for T steps\n",
    "    for t in range(T):\n",
    "        with torch.no_grad():\n",
    "            # forward returns (actions, log_probs, state_values, exploration_var)\n",
    "            a, _, _, _ = policy(states[t])  # shape [N, action_dim]\n",
    "\n",
    "        actions[t] = a\n",
    "\n",
    "        # We must flatten if 'env.vector_step' expects shape (N*action_dim,)\n",
    "        a_np = a.cpu().numpy().reshape(-1)  # shape (N*action_dim,)\n",
    "\n",
    "        # Step environment\n",
    "        next_obs_list, reward_list = env.vector_step(a_np)\n",
    "        # next_obs_list: list of length N, each shape(29,)\n",
    "        # reward_list:   list of length N\n",
    "\n",
    "        # convert to torch\n",
    "        next_obs = torch.tensor(np.stack(next_obs_list), dtype=torch.float32, device=device)\n",
    "        r = torch.tensor(reward_list, dtype=torch.float32, device=device)\n",
    "\n",
    "        # store\n",
    "        states[t+1] = next_obs\n",
    "        rewards[t]  = r\n",
    "        obs = next_obs\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n",
    "def compute_gae_no_done(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards: [T, N]\n",
    "    values:  [T+1, N]\n",
    "    Returns:\n",
    "      advantages:   [T, N]\n",
    "      value_target: [T, N] = advantages + values[:T]\n",
    "    \"\"\"\n",
    "    T, N = rewards.shape\n",
    "    advantages = torch.zeros((T, N), dtype=torch.float32, device=rewards.device)\n",
    "    last_gae = torch.zeros((N,), dtype=torch.float32, device=rewards.device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma*values[t+1] - values[t]\n",
    "        last_gae = delta + gamma*lam*last_gae\n",
    "        advantages[t] = last_gae\n",
    "\n",
    "    value_target = advantages + values[:-1]  # shape [T, N]\n",
    "    return advantages, value_target\n",
    "\n",
    "def ppo_loss(p_ratios, advantages, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    p_ratios:   exp(log_pi(a|s) - log_pi_old(a|s)) [T*N]\n",
    "    advantages: [T*N]\n",
    "    \"\"\"\n",
    "    clipped = torch.clamp(p_ratios, 1.0 - epsilon, 1.0 + epsilon) * advantages\n",
    "    return -torch.min(p_ratios * advantages, clipped).mean()\n",
    "\n",
    "def value_loss(value_preds, value_targets):\n",
    "    \"\"\"\n",
    "    Simple MSE on the value function\n",
    "    \"\"\"\n",
    "    return 0.5 * (value_preds - value_targets).pow(2).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78dce3d",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19265b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo_training():\n",
    "    # Hyperparameters\n",
    "    N = 4         # number of parallel quadrupeds\n",
    "    T = 256         # horizon\n",
    "    epochs = 300    # training epochs\n",
    "    gamma = 0.95\n",
    "    lam = 0.97\n",
    "    epsilon = 0.2\n",
    "    sgd_iters = 5\n",
    "    lr = 1e-3       # Adam learning rate\n",
    "\n",
    "    best_reward = 0\n",
    "\n",
    "    # 1) Create environment\n",
    "    config = {\n",
    "        'N': N,\n",
    "        'vis': False,\n",
    "        'track': 0,\n",
    "        'reward_fcn': walker_reward\n",
    "    }\n",
    "    env = WalkerEnv(config)\n",
    "\n",
    "    # 2) Create bigger policy\n",
    "    state_dim = 29\n",
    "    action_dim = 8\n",
    "    policy = WalkerPolicy(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "\n",
    "    # 3) For logging\n",
    "    mean_rewards_list = []\n",
    "    policy_loss_list  = []\n",
    "    value_loss_list   = []\n",
    "\n",
    "    # 4) Main training loop with tqdm\n",
    "    pbar = tqdm(range(epochs), desc='Training PPO')\n",
    "    for epoch in pbar:\n",
    "        # (a) Collect rollouts\n",
    "        states, actions, rewards = sample_trajectories(env, policy, T, device=device)\n",
    "        # shapes:\n",
    "        #   states:  [T+1, N, 29]\n",
    "        #   actions: [T,   N, 8]\n",
    "        #   rewards: [T,   N]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Evaluate old log-probs and values\n",
    "            T_plus_1, N_ = states.shape[0], states.shape[1]\n",
    "            all_values = policy.value_estimates(states.view(-1, state_dim))  # -> [(T+1)*N, 1]\n",
    "            all_values = all_values.view(T_plus_1, N_)\n",
    "            \n",
    "            states_flat  = states[:-1].reshape(-1, state_dim)  # [T*N, state_dim]\n",
    "            actions_flat = actions.reshape(-1, action_dim)      # [T*N, action_dim]\n",
    "            logp_old = policy.log_prob(actions_flat, states_flat)  # [T*N]\n",
    "\n",
    "        # (b) GAE\n",
    "        advantages, value_targets = compute_gae_no_done(\n",
    "            rewards, all_values, gamma=gamma, lam=lam\n",
    "        )\n",
    "        # Flatten\n",
    "        adv_flat = advantages.view(-1)\n",
    "        val_targ_flat = value_targets.view(-1)\n",
    "\n",
    "        # Normalize advantages\n",
    "        adv_mean = adv_flat.mean()\n",
    "        adv_std  = adv_flat.std() + 1e-8\n",
    "        adv_flat = (adv_flat - adv_mean) / adv_std\n",
    "\n",
    "        # (c) PPO update\n",
    "        for _ in range(sgd_iters):\n",
    "            logp = policy.log_prob(actions_flat, states_flat)  # [T*N]\n",
    "            p_ratios = torch.exp(logp - logp_old)\n",
    "            L_ppo = ppo_loss(p_ratios, adv_flat, epsilon=epsilon)\n",
    "\n",
    "            # Value\n",
    "            new_values = policy.value_estimates(states.view(-1, state_dim)).view(T_plus_1, N_)\n",
    "            new_values_t = new_values[:-1].reshape(-1)\n",
    "            L_v = value_loss(new_values_t, val_targ_flat)\n",
    "\n",
    "            # (Optional) Exploration penalty\n",
    "            # E.g. measure variance of the Beta distribution on states_flat\n",
    "            exploration_penalty_coef = 0.01\n",
    "            alpha_beta = policy.actor_network(states_flat)\n",
    "            alpha, beta = torch.chunk(alpha_beta, 2, dim=-1)\n",
    "            alpha = F.softplus(alpha) + 1.0\n",
    "            beta  = F.softplus(beta)  + 1.0\n",
    "            var_each_dim = (alpha * beta) / ((alpha + beta)**2 * (alpha + beta + 1.0))\n",
    "            exploration_var = var_each_dim.mean()\n",
    "            L_exploration = exploration_penalty_coef * exploration_var\n",
    "\n",
    "            total_loss = L_ppo + L_v + L_exploration\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # (d) Logging for each epoch\n",
    "        ep_mean_reward = rewards.mean().item()\n",
    "        if ep_mean_reward > best_reward:\n",
    "            best_reward = ep_mean_reward\n",
    "            policy.save_weights()\n",
    "            print(f\"New best reward: {best_reward}, saved the model\")\n",
    "        mean_rewards_list.append(ep_mean_reward)\n",
    "        policy_loss_list.append(L_ppo.item())\n",
    "        value_loss_list.append(L_v.item())\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Epoch': epoch, \n",
    "            'MeanReward': f'{ep_mean_reward:.3f}', \n",
    "            'PolicyLoss': f'{L_ppo.item():.3f}', \n",
    "            'ValueLoss': f'{L_v.item():.3f}'\n",
    "        })\n",
    "\n",
    "    # 5) End of training\n",
    "    #policy.save_weights()\n",
    "    env.close()\n",
    "\n",
    "    # 6) Plot results\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Mean Rewards\")\n",
    "    plt.plot(mean_rewards_list, label='Reward')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Policy Loss\")\n",
    "    plt.plot(policy_loss_list, label='Policy Loss', color='orange')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Value Loss\")\n",
    "    plt.plot(value_loss_list, label='Value Loss', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mean_rewards_list, policy_loss_list, value_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57533646",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cda89a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:  12%|█▏        | 37/300 [00:33<03:02,  1.44it/s, Epoch=36, MeanReward=0.013, PolicyLoss=-0.008, ValueLoss=0.734] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 0.013285979628562927, saved the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:  14%|█▎        | 41/300 [00:37<03:57,  1.09it/s, Epoch=40, MeanReward=0.021, PolicyLoss=-0.013, ValueLoss=1.003] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 0.021214595064520836, saved the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:  23%|██▎       | 70/300 [01:03<03:30,  1.09it/s, Epoch=69, MeanReward=-0.053, PolicyLoss=-0.013, ValueLoss=10.301]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_rewards, p_losses, v_losses \u001b[38;5;241m=\u001b[39m \u001b[43mrun_ppo_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 42\u001b[0m, in \u001b[0;36mrun_ppo_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining PPO\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# (a) Collect rollouts\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     states, actions, rewards \u001b[38;5;241m=\u001b[39m \u001b[43msample_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# shapes:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m#   states:  [T+1, N, 29]\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m#   actions: [T,   N, 8]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m#   rewards: [T,   N]\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m# Evaluate old log-probs and values\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36msample_trajectories\u001b[0;34m(env, policy, T, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# forward returns (actions, log_probs, state_values, exploration_var)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m         a, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape [N, action_dim]\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     actions[t] \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# We must flatten if 'env.vector_step' expects shape (N*action_dim,)\u001b[39;00m\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/WalkerPolicy.py:51\u001b[0m, in \u001b[0;36mWalkerPolicy.forward\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     49\u001b[0m dist \u001b[38;5;241m=\u001b[39m Beta(alpha, beta)\n\u001b[1;32m     50\u001b[0m actions \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()                           \u001b[38;5;66;03m# [N, action_dim]\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# [N]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Critic forward\u001b[39;00m\n\u001b[1;32m     54\u001b[0m state_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_network(states)         \u001b[38;5;66;03m# [N, 1]\u001b[39;00m\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/distributions/beta.py:83\u001b[0m, in \u001b[0;36mBeta.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_sample(value)\n\u001b[0;32m---> 83\u001b[0m heads_tails \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dirichlet\u001b[38;5;241m.\u001b[39mlog_prob(heads_tails)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mean_rewards, p_losses, v_losses = run_ppo_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc6623",
   "metadata": {},
   "source": [
    "## Visualise the trained quadruped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e64a34ffdb26d39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:46.270913500Z",
     "start_time": "2023-11-30T09:51:46.157914600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilcsimo/CVUT/UROB/hw5-rl/WalkerPolicy.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n",
      "/tmp/ipykernel_15309/788223384.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  obs_tensor = torch.tensor(obs, dtype=torch.float32)  # shape (4, 29)\n",
      "/home/pilcsimo/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/glfw/__init__.py:917: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 steps, the maximum x value reached was 0.07585714012384415\n"
     ]
    }
   ],
   "source": [
    "# This is the configuration for the Walker environment\n",
    "# N is the number of robots controlled in parallel\n",
    "# vis is a boolean flag to enable visualization\n",
    "# !! IMPORTANT track is an integer index to enable camera tracking of a particular robot (indexed by the value of the argument), this is useful when evaluating the performance of the policy after training\n",
    "# reward_fcn is the reward function that the environment will use to calculate the reward\n",
    "T = 1000\n",
    "x = -1000\n",
    "N = 1\n",
    "\n",
    "# Suppose you have N=4\n",
    "env = WalkerEnv({'N': N, 'vis': VISUALIZE, \"track\": 0, \"reward_fcn\": walker_reward})\n",
    "obs = env.vector_reset()  # shape (4, 29)\n",
    "policy = WalkerPolicy()\n",
    "policy.load_weights()\n",
    "\n",
    "for i in range(T):\n",
    "    # obs is now shape (4, 29)\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)  # shape (4, 29)\n",
    "    actions_tensor = policy.determine_actions(obs_tensor)  # shape (4, 8)\n",
    "    # Flatten to (32,) for env.vector_step\n",
    "    actions_np = actions_tensor.cpu().numpy().reshape(-1)\n",
    "    obs, reward = env.vector_step(actions_np)  # returns new obs, reward for all 4 walkers\n",
    "\n",
    "    # For tracking the maximum x across all walkers:\n",
    "    # obs is a list of length 4, each a (29,)-shaped array\n",
    "    for w in range(N):\n",
    "        x = max(x, obs[w][0])\n",
    "\n",
    "env.close()\n",
    "print(f\"After {T} steps, the maximum x value reached was {x}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
