{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c99faf649cd52b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Teaching a quadruped to walk\n",
    "\n",
    "Time to try out the learning algorithms that you just implemented on a more difficult problem. The WalkerEnv implements a quadruped robot kind-of thing, see for yourself. The goal is to move in the $x$ direction as fast and as far as possible.\n",
    "\n",
    "Your goal is to implement a class `WalkerPolicy` with function `determine_actions()` just like the StochasticPolicy we used earlier to control the pendulum. Below is a template of this class, but feel free to alter it however you want. The only important thing is the `determine_actions()` function!\n",
    "\n",
    "After you implement it, copy `WalkerPolicy` into a separate file `WalkerPolicy.py` that you will upload to BRUTE together with the (optional) learned weights in a zip file. How the policy is implemented is up to you! You are constrained to only the libraries we used so far though, such as torch, numpy etc..\n",
    "\n",
    "You will get some free points just for uploading a working policy (irrelevant of the performance). Further 2 points will be awarded for successfully traversing a small distance in the x direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41290d3f9ccf033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hints\n",
    "\n",
    "There is no single easy way of doing this, but here are some suggestions on what you could try to improve your policy:\n",
    "\n",
    "1. This problem is much more difficult, than balancing a pendulum. It is a good idea to use a bit larger network than for the pendulum policy.\n",
    "\n",
    "2. You can also try to use a different optimizer, such as Adam and play with the hyperparameters.\n",
    "\n",
    "3. Using a neural network to compute the normal distribution scale $\\sigma$ can lead to too much randomness in the actions (i.e. exploration). You can use a fixed $\\sigma$ instead, or replace it with a learnable `torch.Parameter` initialized to some small constant. Make sure, you run it through an exponential, or softplus function to ensure $\\sigma$ is positive.\n",
    "\n",
    "4. The exploration can also be reduced by penalizing the variance of the action distribution in an additional loss term.\n",
    "\n",
    "5. If you see some undesirable behaviour, you can tweak the reward function to penalize it. Even though the $x$ distance is all we care about, adding extra terms to the reward can help guide the learning process (This is known as reward shaping). Simply define a reward function mapping the state $s_{t+1}$ and action $a_t$ to a scalar reward $r_t$ and put it in the config dictionary under the key `'reward_fcn'`. See the `WalkerEnv` class for the implementation of the default reward.\n",
    "\n",
    "6. Using the normal distribution on a bounded action space can lead to certain problems caused by action clipping. This can be mitigated by using a different distribution, such as the Beta distribution. See the `torch.distributions.beta` module for more information. (Note that Beta distribution is defined on the interval [0,1] and works better with parameters $\\alpha,\\beta \\geq 1$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot run with the visualization, you can set this to False\n",
    "VISUALIZE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec37af",
   "metadata": {},
   "source": [
    "### import self-made modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52d6512e1dc81e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:30:41.415964800Z",
     "start_time": "2023-11-30T16:30:40.816557700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy\n",
    "import solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6646694",
   "metadata": {},
   "source": [
    "### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac22145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_MULTIPLIER = 1\n",
    "VELOCITY_MULTIPLIER = 1\n",
    "ACTION_PENALTY_MULTIPLIER = 0\n",
    "STABILITY_PENALTY_MULTIPLIER = 0.25\n",
    "\n",
    "def walker_reward(state, action):\n",
    "    pos = state[:15]  # first 15 elements of state vector are generalized coordinates [xyz, quat, joint_angles]\n",
    "    vel = state[15:]  # last 14 elements of state vector are generalized velocities [xyz_vel, omega, joint_velocities]\n",
    "    x_velocity = vel[0]  # this is the x axis velocity\n",
    "    x_distance = pos[0]  # this is the x axis position\n",
    "    stability_penalty = np.sum(np.abs(vel[1:3]))  # Penalize y and z velocities\n",
    "    action_penalty = np.sum(np.square(action))  # Penalize large actions\n",
    "    return (x_distance * DISTANCE_MULTIPLIER + \n",
    "            x_velocity * VELOCITY_MULTIPLIER - \n",
    "            STABILITY_PENALTY_MULTIPLIER * stability_penalty - \n",
    "            ACTION_PENALTY_MULTIPLIER * action_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1463d",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd317d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvis\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward_fcn\u001b[39m\u001b[38;5;124m\"\u001b[39m: walker_reward}\n\u001b[1;32m     88\u001b[0m env \u001b[38;5;241m=\u001b[39m WalkerEnv(config)\n\u001b[0;32m---> 89\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m policy\u001b[38;5;241m.\u001b[39msave_weights()\n",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, policy, optimizer, num_iterations, gamma, epsilon, sgd_iters)\u001b[0m\n\u001b[1;32m     43\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m L_ppo \u001b[38;5;241m+\u001b[39m L_v\n\u001b[1;32m     45\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m mean_rewards[iteration] \u001b[38;5;241m=\u001b[39m rew_tensor\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/CVUT/UROB/hw5-rl/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "def train(env, policy, optimizer, num_iterations=500, gamma=0.99, epsilon=0.2, sgd_iters=5):\n",
    "    mean_rewards, p_losses, v_losses = np.zeros(num_iterations), np.zeros(num_iterations), np.zeros(num_iterations)  # for logging mean rewards over epochs\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # 1. Rollout\n",
    "        observations, actions, rewards, log_probs, values = [], [], [], [], []\n",
    "        obs = env.vector_reset()\n",
    "        for _ in range(512):  # Rollout for 512 steps\n",
    "            obs = np.array(obs[0])  # Ensure obs is a numpy array\n",
    "            obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "            action, logp, value = policy(obs_tensor)\n",
    "            action = action.squeeze(0).numpy()  # Remove batch dimension and convert to numpy\n",
    "            next_obs, reward = env.vector_step(action)\n",
    "            reward = walker_reward(obs, action)  # Incorporate action into the reward\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(logp)\n",
    "            values.append(value)\n",
    "            obs = next_obs\n",
    "\n",
    "        # 2. Convert data to tensors\n",
    "        obs_tensor = torch.tensor(np.array(observations), dtype=torch.float32)\n",
    "        act_tensor = torch.tensor(np.array(actions), dtype=torch.float32)\n",
    "        rew_tensor = torch.tensor(np.array(rewards), dtype=torch.float32)\n",
    "        logp_tensor = torch.stack(log_probs)\n",
    "        values_tensor = torch.stack(values).squeeze(-1)\n",
    "\n",
    "        # 3. Compute advantage or returns\n",
    "        returns = discount_cum_sum(rew_tensor, gamma)\n",
    "        returns = returns.unsqueeze(-1)  # Ensure returns have the same shape as values_tensor\n",
    "        advantages = returns - values_tensor\n",
    "\n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # 4. PPO loss\n",
    "        for _ in range(sgd_iters):\n",
    "            logp = policy.log_prob(act_tensor, obs_tensor)\n",
    "            p_ratios = torch.exp(logp - logp_tensor.detach())\n",
    "            L_ppo = ppo_loss(p_ratios, advantages, epsilon)\n",
    "            L_v = value_loss(values_tensor, returns)\n",
    "            total_loss = L_ppo + L_v\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        mean_rewards[iteration] = rew_tensor.mean().item()\n",
    "        v_losses[iteration] = L_v.item()\n",
    "        p_losses[iteration] = L_ppo.item()\n",
    "\n",
    "        # 5. Logging and plotting\n",
    "        if iteration % 10 == 0:\n",
    "            print(f\"Iteration {iteration}: Loss = {total_loss.item()}, Mean reward = {mean_rewards[iteration]}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure()\n",
    "    plt.plot(mean_rewards, label='Mean Reward')\n",
    "    plt.plot(p_losses, label='Policy Loss')\n",
    "    plt.plot(v_losses, label='Value Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def discount_cum_sum(rewards, gamma):\n",
    "    discounted_sum = 0\n",
    "    discounted_rewards = []\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_sum = reward + gamma * discounted_sum\n",
    "        discounted_rewards.insert(0, discounted_sum)\n",
    "    return torch.tensor(discounted_rewards, dtype=torch.float32)\n",
    "\n",
    "def ppo_loss(p_ratios, advantages, epsilon):\n",
    "    surr1 = p_ratios * advantages\n",
    "    surr2 = torch.clamp(p_ratios, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    return -torch.min(surr1, surr2).mean()\n",
    "\n",
    "def value_loss(values, returns):\n",
    "    return F.mse_loss(values, returns)\n",
    "\n",
    "policy = WalkerPolicy()\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "config = {'N': 1, 'vis': False, \"track\": 0, \"reward_fcn\": walker_reward}\n",
    "env = WalkerEnv(config)\n",
    "train(env, policy, optimizer, num_iterations=500)\n",
    "policy.save_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc6623",
   "metadata": {},
   "source": [
    "## Visualise the trained quadruped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a34ffdb26d39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:46.270913500Z",
     "start_time": "2023-11-30T09:51:46.157914600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pilcsimo/CVUT/UROB/hw5-rl/WalkerPolicy.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1000 steps, the maximum x value reached was 0.0022255179937928915\n"
     ]
    }
   ],
   "source": [
    "# This is the configuration for the Walker environment\n",
    "# N is the number of robots controlled in parallel\n",
    "# vis is a boolean flag to enable visualization\n",
    "# !! IMPORTANT track is an integer index to enable camera tracking of a particular robot (indexed by the value of the argument), this is useful when evaluating the performance of the policy after training\n",
    "# reward_fcn is the reward function that the environment will use to calculate the reward\n",
    "\n",
    "T = 1000\n",
    "x = -1000\n",
    "env = WalkerEnv({'N': 1, 'vis': VISUALIZE, \"track\": 0, \"reward_fcn\": walker_reward})\n",
    "obs = env.vector_reset()  # Observation vector is of shape (N, 29)\n",
    "POLICY_PATH = \"walker_policy.pth\"\n",
    "policy = WalkerPolicy(load_weights=True)\n",
    "for i in range(1000):\n",
    "    obs = torch.tensor(obs[0], dtype=torch.float32).unsqueeze(0)\n",
    "    # THIS COULD BE USEFUL: a = np.random.randn(1, 8) * 4 - 2  # Random actions with standard deviation of 2 and mean of 0\n",
    "    obs, reward = env.vector_step(policy.determine_actions(obs))\n",
    "    x = max(x, obs[0][0])\n",
    "env.close()\n",
    "print(f\"After {T} steps, the maximum x value reached was {x}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
