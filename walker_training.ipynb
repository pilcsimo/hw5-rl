{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff36817858940fd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:36.674524500Z",
     "start_time": "2023-11-30T09:51:36.663524700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c99faf649cd52b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Teaching a quadruped to walk\n",
    "\n",
    "Time to try out the learning algorithms that you just implemented on a more difficult problem. The WalkerEnv implements a quadruped robot kind-of thing, see for yourself. The goal is to move in the $x$ direction as fast and as far as possible.\n",
    "\n",
    "Your goal is to implement a class `WalkerPolicy` with function `determine_actions()` just like the StochasticPolicy we used earlier to control the pendulum. Below is a template of this class, but feel free to alter it however you want. The only important thing is the `determine_actions()` function!\n",
    "\n",
    "After you implement it, copy `WalkerPolicy` into a separate file `WalkerPolicy.py` that you will upload to BRUTE together with the (optional) learned weights in a zip file. How the policy is implemented is up to you! You are constrained to only the libraries we used so far though, such as torch, numpy etc..\n",
    "\n",
    "You will get some free points just for uploading a working policy (irrelevant of the performance). Further 2 points will be awarded for successfully traversing a small distance in the x direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41290d3f9ccf033",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hints\n",
    "\n",
    "There is no single easy way of doing this, but here are some suggestions on what you could try to improve your policy:\n",
    "\n",
    "1. This problem is much more difficult, than balancing a pendulum. It is a good idea to use a bit larger network than for the pendulum policy.\n",
    "\n",
    "2. You can also try to use a different optimizer, such as Adam and play with the hyperparameters.\n",
    "\n",
    "3. Using a neural network to compute the normal distribution scale $\\sigma$ can lead to too much randomness in the actions (i.e. exploration). You can use a fixed $\\sigma$ instead, or replace it with a learnable `torch.Parameter` initialized to some small constant. Make sure, you run it through an exponential, or softplus function to ensure $\\sigma$ is positive.\n",
    "\n",
    "4. The exploration can also be reduced by penalizing the variance of the action distribution in an additional loss term.\n",
    "\n",
    "5. If you see some undesirable behaviour, you can tweak the reward function to penalize it. Even though the $x$ distance is all we care about, adding extra terms to the reward can help guide the learning process (This is known as reward shaping). Simply define a reward function mapping the state $s_{t+1}$ and action $a_t$ to a scalar reward $r_t$ and put it in the config dictionary under the key `'reward_fcn'`. See the `WalkerEnv` class for the implementation of the default reward.\n",
    "\n",
    "6. Using the normal distribution on a bounded action space can lead to certain problems caused by action clipping. This can be mitigated by using a different distribution, such as the Beta distribution. See the `torch.distributions.beta` module for more information. (Note that Beta distribution is defined on the interval [0,1] and works better with parameters $\\alpha,\\beta \\geq 1$.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f0cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you cannot run with the visualization, you can set this to False\n",
    "VISUALIZE = True\n",
    "# set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec37af",
   "metadata": {},
   "source": [
    "### import self-made modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52d6512e1dc81e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T16:30:41.415964800Z",
     "start_time": "2023-11-30T16:30:40.816557700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from environment.WalkerEnv import WalkerEnv\n",
    "from WalkerPolicy import WalkerPolicy\n",
    "import solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6646694",
   "metadata": {},
   "source": [
    "### Define reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac22145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECT FLIPPING\n",
    "def rotate_vector_by_quaternion(v, q):\n",
    "    \"\"\"\n",
    "    Rotate the 3D vector v by the quaternion q (w, x, y, z).\n",
    "    Returns the rotated vector as a numpy array of shape (3,).\n",
    "    \n",
    "    The quaternion must be normalized. If not, we do it anyway inside.\n",
    "    \"\"\"\n",
    "    # unpack\n",
    "    w, x, y, z = q\n",
    "    # normalize q just in case\n",
    "    norm_q = np.sqrt(w*w + x*x + y*y + z*z)\n",
    "    w, x, y, z = w/norm_q, x/norm_q, y/norm_q, z/norm_q\n",
    "\n",
    "    # v as quaternion: (0, v.x, v.y, v.z)\n",
    "    vx, vy, vz = v\n",
    "\n",
    "    # quaternion product q*v\n",
    "    #  result = q*v = ( w*0 - x*vx - y*vy - z*vz,\n",
    "    #                   w*vx + x*0 + y*vz - z*vy,\n",
    "    #                   w*vy - x*vz + y*0 + z*vx,\n",
    "    #                   w*vz + x*vy - y*vx + z*0 )\n",
    "    #  but let's do it in a simpler function:\n",
    "    #  p = (0, vx, vy, vz)\n",
    "    #  q * p = (w*0 - x*vx - y*vy - z*vz,\n",
    "    #           w*vx + x*0 + y*vz - z*vy,\n",
    "    #           w*vy - x*vz + y*0 + z*vx,\n",
    "    #           w*vz + x*vy - y*vx + z*0)\n",
    "\n",
    "    # for clarity:\n",
    "    qw = w\n",
    "    qx = x\n",
    "    qy = y\n",
    "    qz = z\n",
    "\n",
    "    # first multiply q * p\n",
    "    tw = - (qx*vx + qy*vy + qz*vz)\n",
    "    tx =   qw*vx + qy*vz - qz*vy\n",
    "    ty =   qw*vy - qx*vz + qz*vx\n",
    "    tz =   qw*vz + qx*vy - qy*vx\n",
    "\n",
    "    # then multiply (q*p)*q^* => (tw, tx, ty, tz) * (qw, -qx, -qy, -qz)\n",
    "    #  result is also a quaternion\n",
    "    # w' = tw*qw - tx*qx - ty*qy - tz*qz\n",
    "    rw = tw*qw - tx*qx - ty*qy - tz*qz\n",
    "    rx = tw*(-qx) + tx*qw + ty*(-qz) - tz*(-qy)\n",
    "    ry = tw*(-qy) - tx*(-qz) + ty*qw + tz*(-qx)\n",
    "    rz = tw*(-qz) + tx*(-qy) - ty*(-qx) + tz*qw\n",
    "\n",
    "    # The rotated vector is the (rx, ry, rz) part\n",
    "    return np.array([rx, ry, rz], dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "DISTANCE_MULTIPLIER = 0\n",
    "VELOCITY_MULTIPLIER = 0\n",
    "ACTION_PENALTY_MULTIPLIER = 0\n",
    "STABILITY_PENALTY_MULTIPLIER = 1\n",
    "\n",
    "def walker_reward(state, action):\n",
    "    pos = state[:15]  # first 15 elements of state vector are generalized coordinates [xyz, quat, joint_angles]\n",
    "    vel = state[15:]  # last 14 elements of state vector are generalized velocities [xyz_vel, omega, joint_velocities]\n",
    "    x_velocity = vel[0]  # this is the x axis velocity\n",
    "    x_distance = pos[0]  # this is the x axis position\n",
    "    stability_penalty = np.sum(np.abs(vel[1:3]))  # Penalize y and z velocities\n",
    "    action_penalty = np.sum(np.square(action))  # Penalize large actions\n",
    "    return (x_distance * DISTANCE_MULTIPLIER + \n",
    "            x_velocity * VELOCITY_MULTIPLIER - \n",
    "            STABILITY_PENALTY_MULTIPLIER * stability_penalty - \n",
    "            ACTION_PENALTY_MULTIPLIER * action_penalty)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# partial reward shaping for the Walker environment\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "def sample_trajectories(env, policy, T, device='cpu'):\n",
    "    obs_list = env.reset()[0]  \n",
    "    N = len(obs_list)\n",
    "    state_dim = len(obs_list[0])\n",
    "    action_dim = 8\n",
    "\n",
    "    flip_penalty = 0.0\n",
    "\n",
    "    obs = torch.tensor(np.stack(obs_list), dtype=torch.float32, device=device)\n",
    "    states  = torch.zeros((T+1, N, state_dim), device=device)\n",
    "    actions = torch.zeros((T,   N, action_dim), device=device)\n",
    "    rewards = torch.zeros((T,   N),            device=device)\n",
    "\n",
    "    states[0] = obs\n",
    "\n",
    "    local_up = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "\n",
    "    for t in range(T):\n",
    "        with torch.no_grad():\n",
    "            a, _, _, _ = policy(states[t])\n",
    "\n",
    "        actions[t] = a\n",
    "        a_np = a.cpu().numpy().reshape(-1)\n",
    "\n",
    "        next_obs_list, env_reward_list = env.vector_step(a_np)\n",
    "        next_obs = torch.tensor(np.stack(next_obs_list), dtype=torch.float32, device=device)\n",
    "\n",
    "        custom_rewards = []\n",
    "        for i in range(N):\n",
    "            # ------------------------- PENALISE FLIPPING -------------------------\n",
    "            # parse the quaternion:\n",
    "            q_wxyz = next_obs_list[i][3:7]  # [w, x, y, z]\n",
    "            # rotate local_up:\n",
    "            up_world = rotate_vector_by_quaternion(local_up, q_wxyz)\n",
    "            # if up_world.z < 0 => flipped\n",
    "            if up_world[2] < -0.9:\n",
    "                flip_penalty = -5.0\n",
    "                #print(f\"Robot {i} flipped at time {t}\") # debug if this code works\n",
    "            # -------------------------REWARD ------------------------\n",
    "            x_pos = next_obs_list[i][0]\n",
    "            position_reward = 1.0 * x_pos\n",
    "            x_vel = next_obs_list[i][15]\n",
    "            forward_reward = 10.0 * x_vel\n",
    "            # ---------------------- PENALISE GOING BACKWARDS MORE --------------------\n",
    "            if x_vel < 0.0:\n",
    "                forward_reward *= 1.5\n",
    "            # ---------------------- PENALISE BEING STILL --------------------\n",
    "            if np.abs(x_vel) < 0.05:\n",
    "                forward_reward *= 0\n",
    "\n",
    "            new_r = forward_reward + position_reward + 0.1*env_reward_list[i] if flip_penalty == 0.0 else flip_penalty + position_reward\n",
    "            custom_rewards.append(new_r)\n",
    "            flip_penalty = 0.0\n",
    "\n",
    "        r = torch.tensor(custom_rewards, dtype=torch.float32, device=device)\n",
    "        states[t+1] = next_obs\n",
    "        rewards[t]  = r\n",
    "\n",
    "    return states, actions, rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d1463d",
   "metadata": {},
   "source": [
    "## Train loop\n",
    "### helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd317d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gae_no_done(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    rewards: [T, N]\n",
    "    values:  [T+1, N]\n",
    "    Returns:\n",
    "      advantages:   [T, N]\n",
    "      value_target: [T, N] = advantages + values[:T]\n",
    "    \"\"\"\n",
    "    T, N = rewards.shape\n",
    "    advantages = torch.zeros((T, N), dtype=torch.float32, device=rewards.device)\n",
    "    last_gae = torch.zeros((N,), dtype=torch.float32, device=rewards.device)\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma*values[t+1] - values[t]\n",
    "        last_gae = delta + gamma*lam*last_gae\n",
    "        advantages[t] = last_gae\n",
    "\n",
    "    value_target = advantages + values[:-1]  # shape [T, N]\n",
    "    return advantages, value_target\n",
    "\n",
    "def ppo_loss(p_ratios, advantages, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    p_ratios:   exp(log_pi(a|s) - log_pi_old(a|s)) [T*N]\n",
    "    advantages: [T*N]\n",
    "    \"\"\"\n",
    "    clipped = torch.clamp(p_ratios, 1.0 - epsilon, 1.0 + epsilon) * advantages\n",
    "    return -torch.min(p_ratios * advantages, clipped).mean()\n",
    "\n",
    "def value_loss(value_preds, value_targets):\n",
    "    \"\"\"\n",
    "    Simple MSE on the value function\n",
    "    \"\"\"\n",
    "    return 0.5 * (value_preds - value_targets).pow(2).mean()\n",
    "\n",
    "\n",
    "def visualize_policy_progress(\n",
    "    epoch: int,\n",
    "    policy,\n",
    "    device: str = \"cpu\",\n",
    "    steps: int = 256,\n",
    "    vis_env_config: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes the policy's behavior by running a short rollout \n",
    "    in a single environment with visualization turned on.\n",
    "    \n",
    "    Args:\n",
    "        epoch (int): current training epoch (for logging/title)\n",
    "        policy: your actor-critic policy (with .determine_actions or forward)\n",
    "        device (str): \"cpu\" or \"cuda\"\n",
    "        steps (int): how many steps to roll out for visualization\n",
    "        vis_env_config (dict): environment config dict, e.g. {'N': 1, 'vis': True, ...}\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Create or reuse an environment specifically for visualization\n",
    "    #    We'll assume your environment constructor takes a config dict.\n",
    "    if vis_env_config is None:\n",
    "        # default minimal config\n",
    "        vis_env_config = {\n",
    "            'N': 1,         # single walker\n",
    "            'vis': True,    # turn on visualization\n",
    "            'track': 0      # track robot index 0\n",
    "            # 'reward_fcn': walker_reward, # if needed\n",
    "        }\n",
    "    eval_env = WalkerEnv(vis_env_config)\n",
    "\n",
    "    # 2. Reset environment and prepare for rollout\n",
    "    obs_list = eval_env.reset()  # returns list of size N=1, each obs is shape (29,)\n",
    "    obs = torch.tensor(obs_list[0], dtype=torch.float32, device=device).unsqueeze(0)  \n",
    "    # shape [1, 29]\n",
    "\n",
    "    # 3. We'll collect x-positions (pos[0]) over time to see how far it travels\n",
    "    x_positions = []\n",
    "\n",
    "    # 4. Roll out policy\n",
    "    for t in range(steps):\n",
    "        # a) Determine action from the policy\n",
    "        with torch.no_grad():\n",
    "            action_tensor = policy.determine_actions(obs)  # shape [1, action_dim]\n",
    "        action_np = action_tensor.cpu().numpy().reshape(-1)  # shape (action_dim,)\n",
    "\n",
    "        # b) Step environment\n",
    "        next_obs_list, reward_list = eval_env.vector_step(action_np)\n",
    "        # next_obs_list: length=1, each shape(29,)\n",
    "\n",
    "        # c) Convert next_obs to torch\n",
    "        next_obs = torch.tensor(next_obs_list[0], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # d) Record x-position for debugging\n",
    "        x_position = next_obs_list[0][0]  # the x-coord is pos[0]\n",
    "        x_positions.append(x_position)\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "    # 5. Close the environment (if it requires manual close)\n",
    "    eval_env.close()\n",
    "\n",
    "    # 6. Visualize or print the results\n",
    "    #    For example, we can plot the x-positions over time\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(x_positions, label='x position')\n",
    "    plt.title(f\"Policy Visualization at Epoch {epoch}\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(\"X Position\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78dce3d",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19265b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ppo_training_debug(refine=False):\n",
    "    # ---------------------------------------------------------\n",
    "    # Hyperparameters\n",
    "    # ---------------------------------------------------------\n",
    "    N = 4\n",
    "    T = 512\n",
    "    epochs = 500\n",
    "    gamma = 0.95\n",
    "    lam   = 0.97\n",
    "    epsilon = 0.25\n",
    "    sgd_iters = 5\n",
    "\n",
    "    actor_lr  = 0.001\n",
    "    critic_lr = 0.001\n",
    "    entropy_coef = 0.1\n",
    "    exploration_penalty_start = 0.0\n",
    "    exploration_penalty_end   = 0.0\n",
    "    smoothness_coef = 0.1\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Environment & Policy\n",
    "    # ---------------------------------------------------------\n",
    "    config = {\n",
    "        'N': N,\n",
    "        'vis': False,\n",
    "        'track': 0,\n",
    "        'reward_fcn': walker_reward\n",
    "    }\n",
    "    env = WalkerEnv(config)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"[DEBUG] Using device:\", device)\n",
    "\n",
    "    state_dim = 29\n",
    "    action_dim = 8\n",
    "    policy = WalkerPolicy(state_dim, action_dim).to(device)\n",
    "\n",
    "    if refine:\n",
    "        policy.load_weights()\n",
    "\n",
    "    # Separate actor & critic params\n",
    "    actor_params, critic_params = [], []\n",
    "    for name, param in policy.named_parameters():\n",
    "        if \"actor_network\" in name:\n",
    "            actor_params.append(param)\n",
    "        else:\n",
    "            critic_params.append(param)\n",
    "\n",
    "    actor_optimizer  = optim.Adam(actor_params,  lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic_params, lr=critic_lr)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Logging arrays\n",
    "    # ---------------------------------------------------------\n",
    "    mean_rewards_list = []\n",
    "    policy_loss_list  = []\n",
    "    value_loss_list   = []\n",
    "\n",
    "    entropies_log      = []\n",
    "    alpha_min_log      = []\n",
    "    alpha_max_log      = []\n",
    "    beta_min_log       = []\n",
    "    beta_max_log       = []\n",
    "    actions_min_log    = []\n",
    "    actions_max_log    = []\n",
    "\n",
    "    best_reward = 0\n",
    "    improved = False\n",
    "\n",
    "    pbar = tqdm(range(epochs), desc=\"PPO Training with Debug\")\n",
    "\n",
    "    for epoch in pbar:\n",
    "        # 1) Collect rollouts (using our custom reward logic in sample_trajectories)\n",
    "        states, actions, rewards = sample_trajectories(env, policy, T, device=device)\n",
    "        # shapes: \n",
    "        #   states:  [T+1, N, state_dim]\n",
    "        #   actions: [T,   N, action_dim]\n",
    "        #   rewards: [T,   N]\n",
    "\n",
    "        # For debugging: let's look at the raw action stats\n",
    "        all_actions_np = actions.cpu().numpy().reshape(-1, action_dim)\n",
    "        actions_min_log.append(all_actions_np.min())\n",
    "        actions_max_log.append(all_actions_np.max())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            values_all = policy.value_estimates(states.view(-1, state_dim)).view(T+1, N)\n",
    "            states_flat  = states[:-1].reshape(T*N, state_dim)\n",
    "            actions_flat = actions.reshape(T*N, action_dim)\n",
    "            logp_old = policy.log_prob(actions_flat, states_flat)\n",
    "\n",
    "        # 3) GAE\n",
    "        advantages, value_targets = compute_gae_no_done(rewards, values_all, gamma=gamma, lam=lam)\n",
    "        adv_flat = advantages.view(T*N)\n",
    "        val_targ_flat = value_targets.view(T*N)\n",
    "\n",
    "        adv_mean = adv_flat.mean()\n",
    "        adv_std  = adv_flat.std() + 1e-8\n",
    "        adv_flat = (adv_flat - adv_mean) / adv_std\n",
    "\n",
    "        # 4) PPO update\n",
    "        fraction = epoch / float(epochs)\n",
    "        exploration_penalty_coef = (exploration_penalty_start \n",
    "                                    + fraction*(exploration_penalty_end - exploration_penalty_start))\n",
    "\n",
    "        for _ in range(sgd_iters):\n",
    "            # a) Policy update\n",
    "            actor_optimizer.zero_grad()\n",
    "            logp = policy.log_prob(actions_flat, states_flat)\n",
    "            ratio = torch.exp(logp - logp_old)\n",
    "            L_ppo = ppo_loss(ratio, adv_flat, epsilon=epsilon)\n",
    "\n",
    "            # distribution stats for debugging\n",
    "            with torch.no_grad():\n",
    "                alpha_beta = policy.actor_network(states_flat)\n",
    "                alpha, beta = torch.chunk(alpha_beta, 2, dim=-1)\n",
    "                alpha = F.softplus(alpha) + 1.0\n",
    "                beta  = F.softplus(beta)  + 1.0\n",
    "\n",
    "                dist = Beta(alpha, beta)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                entropies_log.append(entropy.item())\n",
    "                alpha_min_log.append(alpha.min().item())\n",
    "                alpha_max_log.append(alpha.max().item())\n",
    "                beta_min_log.append(beta.min().item())\n",
    "                beta_max_log.append(beta.max().item())\n",
    "\n",
    "            L_entropy = -entropy_coef * entropy\n",
    "\n",
    "            # exploration penalty is 0 for now\n",
    "            variance_each_dim = (alpha * beta) / ((alpha+beta)**2 * (alpha+beta+1.0))\n",
    "            exploration_var = variance_each_dim.mean()\n",
    "            L_exploration = exploration_penalty_coef * exploration_var\n",
    "\n",
    "            # smoothness penalty\n",
    "            actions_shifted = actions[1:] - actions[:-1]\n",
    "            action_diff_penalty = (actions_shifted**2).mean()\n",
    "            L_smoothness = smoothness_coef * action_diff_penalty\n",
    "\n",
    "            policy_loss = L_ppo + L_exploration + L_smoothness # + L_entropy THIS IS BULLSHIT BECAUSE ENTROPY IS NEGATIVE?????\n",
    "            policy_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # b) Critic update\n",
    "            critic_optimizer.zero_grad()\n",
    "            new_values_all = policy.value_estimates(states.view(-1, state_dim)).view(T+1, N)\n",
    "            new_values_t   = new_values_all[:-1].reshape(T*N)\n",
    "            L_v = value_loss(new_values_t, val_targ_flat)\n",
    "            L_v.backward()\n",
    "            critic_optimizer.step()\n",
    "\n",
    "        # 5) Logging\n",
    "        mean_reward = rewards.mean().item()\n",
    "        mean_rewards_list.append(mean_reward)\n",
    "        policy_loss_list.append(policy_loss.item())\n",
    "        value_loss_list.append(L_v.item())\n",
    "\n",
    "        if mean_reward > best_reward:\n",
    "            best_reward = mean_reward\n",
    "            policy.save_weights()\n",
    "            improved = True\n",
    "            print(f\"[DEBUG] New best mean reward: {best_reward:.3f}, model saved.\")\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'Epoch': epoch,\n",
    "            'MeanReward': f\"{mean_reward:.3f}\",\n",
    "            'PolicyLoss': f\"{policy_loss.item():.4f}\",\n",
    "            'ValueLoss': f\"{L_v.item():.4f}\",\n",
    "            'Entropy': f\"{entropy.item():.3f}\",\n",
    "            'VarPenalty': f\"{exploration_var.item():.3f}\",\n",
    "            'Smoothness': f\"{action_diff_penalty.item():.3f}\"\n",
    "        })\n",
    "\n",
    "        # Optional: visualize every 50 epochs\n",
    "        if (epoch % 50 == 0 and epoch > 0): #or (improved and epoch > 100) :\n",
    "            visualize_policy_progress(\n",
    "                epoch,\n",
    "                policy,\n",
    "                device=device,\n",
    "                steps=T,\n",
    "                vis_env_config={\n",
    "                    'N': 1,\n",
    "                    'vis': True,\n",
    "                    'track': 0,\n",
    "                }\n",
    "            )\n",
    "            improved = False\n",
    "\n",
    "    # end training\n",
    "    env.close()\n",
    "\n",
    "    # Plot all your logs\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.subplot(2,3,1)\n",
    "    plt.plot(mean_rewards_list)\n",
    "    plt.title(\"Mean Reward\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.subplot(2,3,2)\n",
    "    plt.plot(policy_loss_list, color=\"orange\")\n",
    "    plt.title(\"Policy Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.subplot(2,3,3)\n",
    "    plt.plot(value_loss_list, color=\"green\")\n",
    "    plt.title(\"Value Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "\n",
    "    plt.subplot(2,3,4)\n",
    "    plt.plot(entropies_log, label=\"Entropy\")\n",
    "    plt.title(\"Policy Entropy\")\n",
    "    plt.xlabel(\"Update step\")\n",
    "\n",
    "    plt.subplot(2,3,5)\n",
    "    plt.plot(actions_min_log, label=\"Action min\")\n",
    "    plt.plot(actions_max_log, label=\"Action max\")\n",
    "    plt.title(\"Action Range Over Epochs\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2,3,6)\n",
    "    plt.plot(alpha_min_log, label=\"Alpha min\", color=\"red\")\n",
    "    plt.plot(alpha_max_log, label=\"Alpha max\", color=\"blue\")\n",
    "    plt.title(\"Alpha Range Over Updates\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return mean_rewards_list, policy_loss_list, value_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57533646",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rewards, p_losses, v_losses = run_ppo_training_debug(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd7bea",
   "metadata": {},
   "source": [
    "## Debug how the quadruped moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e9b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_wave_actions():\n",
    "    env = WalkerEnv({'N': 1, 'vis': True, 'track': 0})\n",
    "    obs = env.reset()\n",
    "\n",
    "    for step in range(500):\n",
    "        # example: wave each joint from -1 to +1 over time\n",
    "        wave = np.sin(step * 0.3)  # range ~ [-0.5, 0.5]\n",
    "        action = np.array([-2, wave, 2, wave, -2.0, 3, 2.0, 3])  # shape (8,)\n",
    "        obs, rewards = env.vector_step(action)\n",
    "        local_up = np.array([0.0, 0.0, 1.0], dtype=np.float32)\n",
    "        q_wxyz = obs[0][3:7]  # [w, x, y, z]\n",
    "        # rotate local_up:\n",
    "        up_world = rotate_vector_by_quaternion(local_up, q_wxyz)\n",
    "            \n",
    "        # if up_world.z < 0 => flipped\n",
    "        print (f\"step: {step} UW:{up_world[2]}\")\n",
    "        if up_world[2] < -0.9:\n",
    "            print(f\"Robot flipped at time {step}\")\n",
    "        # does the robot's legs wave? do we see any movement?\n",
    "\n",
    "    env.close()\n",
    "\n",
    "debug_wave_actions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cc6623",
   "metadata": {},
   "source": [
    "## Visualise the trained quadruped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a34ffdb26d39b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:51:46.270913500Z",
     "start_time": "2023-11-30T09:51:46.157914600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is the configuration for the Walker environment\n",
    "# N is the number of robots controlled in parallel\n",
    "# vis is a boolean flag to enable visualization\n",
    "# !! IMPORTANT track is an integer index to enable camera tracking of a particular robot (indexed by the value of the argument), this is useful when evaluating the performance of the policy after training\n",
    "# reward_fcn is the reward function that the environment will use to calculate the reward\n",
    "T = 1000\n",
    "x = -1000\n",
    "N = 1\n",
    "\n",
    "env = WalkerEnv({'N': N, 'vis': VISUALIZE, \"track\": 0, \"reward_fcn\": walker_reward})\n",
    "obs = env.vector_reset()  # shape (4, 29)\n",
    "policy = WalkerPolicy()\n",
    "policy.load_weights()\n",
    "\n",
    "for i in range(T):\n",
    "    # obs is now shape (4, 29)\n",
    "    obs_tensor = torch.tensor(obs, dtype=torch.float32)  # shape (N, 29)\n",
    "    actions_tensor = policy.determine_actions(obs_tensor)  # shape (N, 8)\n",
    "\n",
    "    # Flatten to (32,) for env.vector_step\n",
    "    actions_np = actions_tensor.cpu().numpy().reshape(-1)\n",
    "    obs, reward = env.vector_step(actions_np)  # returns new obs, reward for all N walkers\n",
    "\n",
    "    # For tracking the maximum x across all walkers:\n",
    "    # obs is a list of length 4, each a (29,)-shaped array\n",
    "    for w in range(N):\n",
    "        x = max(x, obs[w][0])\n",
    "\n",
    "env.close()\n",
    "print(f\"After {T} steps, the maximum x value reached was {x}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
